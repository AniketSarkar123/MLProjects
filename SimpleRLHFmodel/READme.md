RLHF Reward Model Project

This project implements a Reward Model training pipeline for Reinforcement Learning with Human Feedback (RLHF).
It allows you to train a BERT-based reward model using human preference data and evaluate answer quality for Q/A pairs.

ğŸ“‚ Project Structure
RLProject/
â”‚
â”œâ”€â”€ prefs.jsonl              # Human preference dataset (chosen vs rejected answers)
â”œâ”€â”€ rlhf_pipeline.py         # Training pipeline (train reward model)
â”œâ”€â”€ test.py                  # Script to test trained reward model
â”œâ”€â”€ checkpoints/             # Saved models after training
â”‚   â””â”€â”€ reward/              # Reward model checkpoint
â””â”€â”€ README.md                # Project documentation

âš™ï¸ Installation

Clone the repo and install dependencies:

git clone <your-repo-url>
cd RLProject
pip install torch transformers datasets accelerate


(Optional) For GPU training, ensure you have CUDA + PyTorch installed.

ğŸ“Š Dataset Format

The prefs.jsonl file stores human preferences in JSONL format:

{"prompt": "What is the capital of France?", "chosen": "Paris is the capital of France.", "rejected": "France is in Europe."}
{"prompt": "Who wrote Hamlet?", "chosen": "Hamlet was written by William Shakespeare.", "rejected": "It is a movie directed by Christopher Nolan."}


Each entry contains:

prompt â†’ The question/task.

chosen â†’ The preferred (good) answer.

rejected â†’ The less preferred (bad) answer.

ğŸ‹ï¸ Training the Reward Model

Run the following command to train:

python rlhf_pipeline.py --mode train_reward --feedback_file prefs.jsonl --model_name bert-base-uncased --out_dir checkpoints/reward


--mode train_reward â†’ Trains reward model.

--feedback_file â†’ Path to preference dataset.

--model_name â†’ Backbone model (default: bert-base-uncased).

--out_dir â†’ Where to save trained model.

âœ… Testing the Model

After training, test the reward model with test.py:

python test.py



Correct, context-related answers score higher, while irrelevant ones score lower.

ğŸ“Œ Next Steps

Add more human feedback data in prefs.jsonl to improve accuracy.

Fine-tune on larger language models (e.g., RoBERTa, DeBERTa).

Extend pipeline for policy optimization with PPO (next RLHF step).
